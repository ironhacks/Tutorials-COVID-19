{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #2: How to query data from BigQuery in a Notebook\n",
    "\n",
    "Welcome to our second R tutorial designed for participants in the IronHacks. In this notebook, we will show you how you can access our training data stored in BigQuery using a key stored in your user profile.\n",
    "\n",
    "> Before you get started: This tutorial will not more 10 min and you should be able to work with our training data right after. \n",
    "\n",
    "**Our goal**: Help you getting started with the R packages `BigRQuery` or `DBI` or `dply` in order to access a training dataset stored in BigQuery! So if you have never used these libraries before this tutorial is key for you. \n",
    "\n",
    "**BigQuery - What's that?**: \n",
    "BigQuery is Google's flagship data warehousing system: \"Serverless, highly scalable, and cost-effective multi-cloud data warehouse designed for business agility\". It allows you to analyze large amount of data using ANSI SQL at blazing-fast speeds, with zero operational overhead. You can find out more [here](https://cloud.google.com/bigquery). \n",
    "\n",
    "\n",
    "**Why do we use BigQuery?**\n",
    "In the COVID-19 Data Science Challenges you will use BIG DATA from our data providers SafeGraph, the Management Performance Hub (MPH), and other partners (Department of Workforce Development). The first hack Summer 2020, will use preprocessed data so you will not need to use all the functionalities of BIG QUERY as we have sampled down more than 50 datasets with more than 1 TB and millions of raws into a small sets of cleaned tables without missing entries and clear identifiers. However, using BigQuery will still be very helpful as you can see for exploring data without having to use them in memory etc. It will also set you up for the future of data science since BigQuery is replacing other BIG DATA services (e.g. Spark). \n",
    "\n",
    "**How do we give you access to BigQuery?**\n",
    "In Big Query data are stored in projects. Inside a project there are multiple datasets. Each dataset can contain multiple tables. \n",
    "In this hack we give you access to a project called: `ironhacks-covid19-data`. In this project there are two datasets:`ironhacks-covid19-data:ironhacks_covid19_training` and `ironhacks-covid19-data:ironhacks_covid19_competition\n",
    "`. During the training period you will only find data in the first dataset. In this first tutorial we only use one first relatively simply structured table stored in this dataset. It is called `covid19_tests_cases_deaths_IN`\n",
    "\n",
    "> Keep in mind: In this tutorial you will learn how to get access to the `ironhacks-covid19-data` and the datset `ironhacks-covid19-data:ironhacks_covid19_training` stored inside this project.\n",
    "\n",
    "**What's BigRQuery?**: \n",
    "`BigRQuery` package allows you to query data stored in BigQuery You can find the official documentation [here](https://bigrquery.r-dbi.org/)\n",
    "\n",
    "**What's DBI?**\n",
    "The `DBI` package helps connecting R to database management systems (DBMS). `DBI` separates the connectivity to the DBMS into a “front-end” and a “back-end”. It defines an interface that is implemented by the \"backend\". In our case the backend is BigRQuery (but it also uses other backends such as SQLlite etc.). You can find the official documentation [here](https://dbi.r-dbi.org/) \n",
    "\n",
    "**What's dplry?**\n",
    "`dplry` is is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges. It is part of the `tidyverse` package. You can find the official documentation [here](https://dplyr.tidyverse.org/) The `dplyr` interface lets you treat BigQuery tables as if they are in-memory data frames. This is the most convenient layer if you don’t want to write SQL, but instead want dbplyr to perform SQL queries in the background.\n",
    "\n",
    "> the following tutorial is structured in four section! So let's get started! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section I: Loading the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, we will be loading the libraries we need. I load the complete tidyverse package in case I want to also plot later but you can also just load dplyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: bigrquery\n",
      "\n",
      "Loading required package: tidyverse\n",
      "\n",
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.0 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.1     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.0.3     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.1.0     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.0\n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "Loading required package: DBI\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. TRUE\n",
       "2. TRUE\n",
       "3. TRUE\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] TRUE\n",
       "\n",
       "[[2]]\n",
       "[1] TRUE\n",
       "\n",
       "[[3]]\n",
       "[1] TRUE\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lapply (c(\"bigrquery\",\"tidyverse\",\"DBI\"),require, character.only=TRUE) ###loading the libraries we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As mentioned in part 1 of our tutorial, there is no installation required for you. So this should do the job very fast for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section II: Authorizing your BigQuery Access\n",
    "\n",
    "1. Getting your key\n",
    "2. Setting up the autorization\n",
    "3. And finding out more what you can do with this key!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the next step now is to find the keys: 1) Go to your user profile, 2) click on *Download your hack dataset training key* and 3) Upload it to your Juptyer lab environment. Below you find a screenshot where you can find your key. \n",
    "\n",
    "![image](https://raw.githubusercontent.com/ironhacks/Tutorials-COVID-19/master/part-2/profile_image.jpg?token=AC7DAY5X2TFRU6J6IU7ZQ5S7FXKP2)\n",
    "\n",
    "> After you have added the key to your file structure in your lab environment you can authorize your access! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_auth(path = \"/home/jovyan/service-account-3.json\") #authorizing the token! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **This is an important step***: please keep in mind that you need to put the path `/home/jovyan/` followed by your directory name and the name that you give to your json file! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bq_has_token() #confirms that bq has token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Yeah it is working!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'ironhacks-covid19-data'"
      ],
      "text/latex": [
       "'ironhacks-covid19-data'"
      ],
      "text/markdown": [
       "'ironhacks-covid19-data'"
      ],
      "text/plain": [
       "[1] \"ironhacks-covid19-data\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bq_projects(page_size = 10, max_pages = 1, warn = TRUE) # now I am getting the projects that are associated with that token! Yeah - it is the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]\n",
       "<bq_dataset> ironhacks-covid19-data.ironhacks_covid19_competition\n",
       "\n",
       "[[2]]\n",
       "<bq_dataset> ironhacks-covid19-data.ironhacks_covid19_training\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bq_project_datasets('ironhacks-covid19-data', page_size = 10, max_pages = 1, warn = TRUE) # now you also see the datasets in the project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next** we are establishing the database connection! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectid<-'ironhacks-covid19-data'\n",
    "datasetid<-'ironhacks_covid19_training'\n",
    "bq_conn <-  DBI::dbConnect(bigquery(), \n",
    "                            project = projectid,\n",
    "                            dataset = datasetid,\n",
    "                            use_legacy_sql = FALSE\n",
    "                      ) ## setting up the project and the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'covid19_tests_cases_deaths_IN'</li><li>'weather_data'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'covid19\\_tests\\_cases\\_deaths\\_IN'\n",
       "\\item 'weather\\_data'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'covid19_tests_cases_deaths_IN'\n",
       "2. 'weather_data'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"covid19_tests_cases_deaths_IN\" \"weather_data\"                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DBI::dbListTables(bq_conn)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Yeah! Now we have what we need to move on! We have also prepared a sheet with the schema of the database so that it is easier for you to understand what is in there. It can be found [here](https://docs.google.com/spreadsheets/d/e/2PACX-1vRsGyw0gPHuYT8lBZ1l8kHXWTmwecjrDsqYL9jc3dcvu4aodEWlleDKz9JXZ0ANLBhjvR6htEm_C7y7/pubhtml?gid=557548514&single=true). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section III: Loading, understanding, and querying table with dplr() wihout using SQL directly\n",
    "So what we do: \n",
    "* Using 'dplyr' to load the table and understanding it WITHOUT loading into memory using pipes `%>%` and `summarize()` \n",
    "* Querying the table with `dplr` to select the variables `DATE`,`COVID_TEST`,`COVID_COUNT` with the `select()` function, filtering for period `2020-04-01` to `2020-05-01` using the `filter()`function, and summarizing it using `summarize()` - all without loading it into memory\n",
    "* Plotting the query with ggplot() without loading into memory\n",
    "* Eventually loading it into memory as a tibble..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table `covid19_tests_cases_deaths_IN` can be easily accessed by using the dplyr::tbl() function and passing the table name and BigQuery connection details as parameters. We will do this next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table<-dplyr::tbl(bq_conn,'covid19_tests_cases_deaths_IN') ##checking the tables in there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the “table” is of class tbl_sql and not a tibble. This is because tbl() creates a direct reference to the table in BigQuery but does not bring the data in-memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Great work! So now you you want to know a little more about the data (e.g. what is the man and max DATE, what is the mean COVID_TEST, and how many nas). You can explore all this without having to actually load the data in memory. So let's do this now using the power PIPEs from dplyr()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2<-table %>% summarize(mean_tests=mean(COVID_TEST), mean_cases=mean(COVID_COUNT), min_date=min(DATE),max_date=max(DATE),nrows=COUNT(DATE))\n",
    "head(table2) \n",
    "class(table2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note:table2 is also a class of tbl_sql and not a tibble. It is not yet in memory. We can try a bit more and subset/select a certain timeframe and then calcualte the means and also check the time frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table3<-table %>% select(DATE,COVID_TEST,COVID_COUNT) %>% filter(DATE >=\"2020-04-01\" & DATE <=\"2020-05-01\")\n",
    "head(table3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table3 %>% summarize (mean_tests=mean(COVID_TEST),mean_cases=mean(COVID_COUNT),min_date=min(DATE),max_date=max(DATE),nrow=COUNT(COVID_TEST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> even without transforming the data you can easily plot them (without having them in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(table3, aes(DATE,COVID_TEST))+geom_point()\n",
    "ggplot(table3, aes(DATE,COVID_COUNT))+geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, since you have decided what data you want to model, you can pull them into memory, simply by using as_tibble or as.data.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table4<-as_tibble(table3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class(table4)\n",
    "nrow(table4) #and since it is a tibble now you can apply the function nrow() :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Great job! now you could start and you do not even need a single SQL command to get moving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section IV: Loading, understanding, and querying table using DBI and SQL statements\n",
    "So what we will do: \n",
    "* Using the `dbSendQuery()` function in the DBI package \n",
    "    * to send SQL queries to load and understand the table (`SELECT()`, and `MIN()`,`MAX()`,`AVG()`, `COUNT()`) \n",
    "    * to query the table and selecting `DATE`,`COVID_TEST`, `COVID_COUNT` using `SELECT()` for the timeframe `2020-04-01` to `2020-05-01` using `WITH()`\n",
    "    * to summarize the query with `MIN()`,`MAX()`,`AVG()`, `COUNT()` using a nested SQL statement\n",
    "* Using the `dbGetQuery()` function in the DBI package to load the table into memory and plotting it with ggplot2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily load, query and explore the table in BigQuery using the DBI package without directly bringing it into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi_tb<-dbSendQuery(bq_conn,\"SELECT * FROM covid19_tests_cases_deaths_IN\")\n",
    "head(dbFetch(dbi_tb),n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class(dbi_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that \"rs” is of class 'BigQueryResult' and not a tibble. This is because dbSendQuery() and dbFetch() create a direct reference to the table in BigQuery but does NOT bring the data in-memory. This is particularly useful if you work with big data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query<-\"SELECT MIN(DATE) AS MIN_DATE ,MAX(DATE) AS MAX_DATE, AVG(COVID_TEST) AS AVG_TEST,AVG(COVID_COUNT) as AVG_COUNT,COUNT(DATE) AS NROWs FROM covid19_tests_cases_deaths_IN\"\n",
    "dbi_tb<-dbSendQuery(bq_conn,query)\n",
    "dbFetch(dbi_tb)\n",
    "class(dbi_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2<-\"SELECT DATE,COVID_TEST,COVID_COUNT FROM covid19_tests_cases_deaths_IN WHERE (DATE BETWEEN '2020-04-01' AND '2020-05-01')\"\n",
    "dbi_tb2<-dbSendQuery(bq_conn,query2)\n",
    "head(dbFetch(dbi_tb2), n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3<-\"SELECT MIN(DATE) AS MIN_DATE, MAX(DATE) AS MAX_DATE,AVG(COVID_TEST) as mean_test, AVG(COVID_COUNT) as mean_cases,COUNT(DATE) as nrows FROM (SELECT DATE,COVID_TEST,COVID_COUNT FROM covid19_tests_cases_deaths_IN WHERE (DATE BETWEEN '2020-04-01' AND '2020-05-01'))\"\n",
    "dbi_tb3<-dbSendQuery(bq_conn,query3)\n",
    "dbFetch(dbi_tb3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(dbi_tb2, aes(DATE,COVID_TEST))+geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**! You cannot use ggplot when you fetch with DBI direclty as the object class BIGQUERYRESULT is not recognized...So let's try something else!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi_tb2b<-dbGetQuery(bq_conn,query2)\n",
    "head(dbi_tb2b)\n",
    "class(dbi_tb2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installed.packages()[,\"Package\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(package=\"dplyr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(dbi_tb2b, aes(DATE,COVID_TEST))+geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Great. We got this sorted. so with dbGetQuery you get a dataframe that you can manipulate! and plot! and to make this even more complicated I can also show you that it also works with the bq_project_query() from the BigRQuery package but....\n",
    "> That's it for now and I am sure you can figure this out now! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
