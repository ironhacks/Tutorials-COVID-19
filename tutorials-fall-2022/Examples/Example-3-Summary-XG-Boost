## An Example of the XG-boosting model
### Description of the algorithm
Bagging for decision trees involves bootstrapping multiple samples from the training data, fitting each tree and combining all trees to create a single predictive model. Boosting grows these trees sequentially and in small steps. Given a model, a small decision tree is fit to the residuals from the model, and this tree is added to the function to update residuals. In this way, boosting slowly improves places where the model doesn't perform well. and the learning steps can be small and slow.

### Description of the procedure
After pulling the data, only columns useful for the current modeling are chosen. Training and testing data was split on the the beginning of 2020. Since boosting is trained by minimizing loss of an objective function against a dataset, the choice of loss function is a hyper parameter. After some experiments, regression with squared loss was chosen. Since there are a few parameters to choose, such as the maximum depth of trees, cross-validation was used to search for the optimal parameters. After building the model, prediction was made and prediction errors were calculated.

### Packages used
sklearn
xgboost