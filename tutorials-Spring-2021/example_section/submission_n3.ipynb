{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "submission_prediction_output.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4FlDtuesSQs"
      },
      "source": [
        "\n",
        "%logstop\n",
        "%logstart -t -r -q ipython_command_log.py global\n",
        "\n",
        "#- IRONHACKS RESEARCH TRACKING CODE\n",
        "#----------------------------------\n",
        "# The following code is used to help our research team understand how you \n",
        "# our notebook environment. We do not collect any personal information with\n",
        "# the following code, it is used to measure when and how often you work on\n",
        "# your submission files.\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "import IPython.core.history as history\n",
        "\n",
        "ha = history.HistoryAccessor()\n",
        "ha_tail = ha.get_tail(1)\n",
        "ha_cmd = next(ha_tail)\n",
        "session_id = str(ha_cmd[0])\n",
        "command_id = str(ha_cmd[1])\n",
        "timestamp = datetime.utcnow().isoformat()\n",
        "history_line = ','.join([session_id, command_id, timestamp]) + '\\n'\n",
        "logfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\n",
        "logfile.write(history_line)\n",
        "logfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P1InPQNsSQ1"
      },
      "source": [
        "#- IMPORT THE LIBRARIES \n",
        "#-----------------------\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud.bigquery import magics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2975Af37sSQ1"
      },
      "source": [
        "Funtions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbVMg-E4sSQ2"
      },
      "source": [
        "def query(querys):\n",
        "    \"\"\"\n",
        "    Will return a tuple with pandas dataframes of all the query results\n",
        "    input: list of querys\n",
        "    output: tuple of dataframes\n",
        "    \"\"\"\n",
        "    BIGQUERY_PROJECT = 'ironhacks-covid19-data'\n",
        "    BIGQUERY_KEYPATH = '../home/jovyan/key.json'\n",
        "\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = BIGQUERY_KEYPATH\n",
        "    bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)\n",
        "    \n",
        "    df_results = []\n",
        "    for i, q in enumerate(querys):\n",
        "        df = bigquery_client.query(q).to_dataframe()\n",
        "        df_results.append(df)\n",
        "        num_cols = len(df.columns)\n",
        "        num_rows = len(df)\n",
        "        print(\"Query {a} has {b} elements with {c} features\".format(a=i, c=num_cols, b=num_rows))\n",
        "    return tuple(df_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtLagloKsSQ2"
      },
      "source": [
        "def rollout_dataframe(df, var_cols, rolling_epoch, predict=False):\n",
        "    \"\"\"\n",
        "    Coverts a time series formated dataframe into a dataframe that can be used for training a classifiaction based model type (like random forest)\n",
        "    var_cols are all the column names that are varites you want to use in the new dataframe (can handle uni and multivariate)\n",
        "    rolling_epoch is number time steps per row for training. There are no repeats, so if I have 44 time steps and rolling epoch is 4, new row 1\n",
        "    will be weeks 1,2,3,4 and then weeks 5 label is attached. Row 2 is weeks 6,7,8,9 abd week 10 is the label. can also make the dataframe needed to predict\n",
        "    week 44 counts\n",
        "    input: df=df to roll out; var_cols=column names to rollout; rolling_epoch=number of time steps per row; predict=if you want to make one for predicitng week 44\n",
        "    returns: pandas dataframe\n",
        "    \"\"\"\n",
        "    roll_cols = []\n",
        "    for x in variation_cols:\n",
        "        for y in range(1,rolling_epoch+1):\n",
        "            roll_cols.append(x+str(y))\n",
        "    if not predict:\n",
        "        roll_cols.append('label')\n",
        "    roll_cols = ['latitude', 'longitude'] + roll_cols\n",
        "    \n",
        "    \n",
        "    if predict:\n",
        "        roll_cols = [\"poi_id\"] + roll_cols\n",
        "        df_main = pd.DataFrame([],  columns = roll_cols)\n",
        "        for poi_id in poi_ids:\n",
        "            tmp = df.loc[df[\"poi_id\"]==poi_id].copy()\n",
        "            cols = [tmp.iloc[0]['poi_id'], float(tmp.iloc[0]['latitude']), float(tmp.iloc[0]['longitude'])]\n",
        "            for var in variation_cols:\n",
        "                for y in range(len(tmp)-rolling_epoch, len(tmp)):\n",
        "                    a = tmp.iloc[y][var]\n",
        "                    cols.append(a)\n",
        "            df_main.loc[-1] = cols  \n",
        "            df_main.index = df_main.index + 1 \n",
        "            df_main = df_main.sort_index()  \n",
        "    else:\n",
        "        df_main = pd.DataFrame([],  columns = roll_cols)\n",
        "        for poi_id in poi_ids:\n",
        "            tmp = df.loc[df[\"poi_id\"]==poi_id].copy()\n",
        "            for x in range(0, len(tmp), rolling_epoch+1):\n",
        "                if x + rolling_epoch >= len(tmp):\n",
        "                    break\n",
        "                cols = [float(tmp.iloc[0]['latitude']), float(tmp.iloc[0]['longitude'])]\n",
        "                for var in variation_cols:\n",
        "                    for y in range(x, x+rolling_epoch):\n",
        "                        a = tmp.iloc[y][var]\n",
        "                        cols.append(a)\n",
        "                cols.append(tmp.iloc[x+rolling_epoch]['raw_visit_counts'])\n",
        "                df_main.loc[-1] = cols  \n",
        "                df_main.index = df_main.index + 1\n",
        "                df_main = df_main.sort_index() \n",
        "    return df_main"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MKqIdb5sSQ3"
      },
      "source": [
        "Getting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkXaz9kksSQ4"
      },
      "source": [
        "\"\"\"\n",
        "there has be to be two querys here because for some reason, I found that the social distancing table has lot and lots of duplicate rows (like tens of millions). \n",
        "This makes any query where I try to join in sql to get to large and fail. Therefore, I removed the duplicates and merged them with pandas\n",
        "\"\"\"\n",
        "query1 = \"\"\"\n",
        "         SELECT *\n",
        "         FROM `ironhacks_covid19_competition`.`weekly_patterns`\n",
        "         \"\"\"\n",
        "query2 = \"\"\"\n",
        "         SELECT *\n",
        "         FROM `ironhacks_covid19_competition`.`cbg_social_distancing`\n",
        "         WHERE cbg in (SELECT DISTINCT(poi_cbg) FROM `ironhacks_covid19_competition`.`weekly_patterns`) \n",
        "         \"\"\"\n",
        "(df_weekly, df_social_dist) = query([query1, query2])\n",
        "\n",
        "# drop the duplicates from the social distancing table\n",
        "df_social_dist.drop_duplicates(subset=None, keep=\"first\", inplace=True)\n",
        "\n",
        "# rename the cbg id column so table can be merged\n",
        "df_weekly.rename(columns={'poi_cbg':'cbg'}, inplace=True)\n",
        "\n",
        "# merge dataframes on both the cbg and week_number columns to get final per week merged table\n",
        "df = pd.merge(df_weekly, df_social_dist, on = [\"cbg\", \"week_number\"], how = \"inner\")\n",
        "\n",
        "# print some quick info on the table to make sure it was made right\n",
        "print(\"Columns:\")\n",
        "print('\\n'.join(df.columns))\n",
        "print(\"\\nResults:\")\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmGKVPuBsSQ4"
      },
      "source": [
        "# get a set of all the poi_ids that are being used here\n",
        "poi_ids = list(set(df_rf[\"poi_id\"].tolist()))\n",
        "\n",
        "# check that there are no missing values\n",
        "print(sum(df_main.isna().values))\n",
        "\n",
        "# a quick investigation to see if every poi has the same right number of weeks present\n",
        "print(df.groupby(by='poi_id').agg('count')['week_number'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJoz4WmwsSQ5"
      },
      "source": [
        "# roll out the dataframe so it can be use by things like randomforest, svm adaboost, gradient boost\n",
        "variation_cols = ['raw_visit_counts', 'visits_concentration',\n",
        "       'distance_from_home', 'median_dwell', 'device_count_week',\n",
        "       'completely_home_device_count_per_week',\n",
        "       'median_home_dwell_time_per_week',\n",
        "       'median_non_home_dwell_time_per_week']\n",
        "rolling_epoch = 4\n",
        "\n",
        "df_rolling = rollout_dataframe(df, variation_cols, rolling_epoch)\n",
        "num_cols = len(df_rolling.columns)\n",
        "num_rows = len(df_rolling)\n",
        "print(\"Dataframe has {b} elements with {c} features\".format(c=num_cols, b=num_rows))\n",
        "print(df_rolling.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzX9kvfVsSQ5"
      },
      "source": [
        "num_cols = len(df_rolling.columns)\n",
        "num_rows = len(df_rolling)\n",
        "print(\"Dataframe has {b} elements with {c} features\".format(c=num_cols, b=num_rows))\n",
        "print(df_rolling.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SzyCm2AsSQ5"
      },
      "source": [
        "Building model and hyper parameter search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHhVZj-gsSQ6"
      },
      "source": [
        "# do some hyperparamter searching\n",
        "parameters = {'n_estimators':[100,150,200]}\n",
        "\n",
        "X, y = df_rolling.iloc[:,:-1], df_rolling.iloc[:,-1]\n",
        "rfr = sklearn.ensemble.RandomForestRegressor()\n",
        "rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n",
        "clf = GridSearchCV(rfr, param_grid=parameters, cv=rs)\n",
        "clf.fit(X, y)\n",
        "clf.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4srIFgysSQ6"
      },
      "source": [
        "Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiv3Ww5OsSQ6"
      },
      "source": [
        "# build a predict df for predicitng week 44\n",
        "predict_df = rollout_dataframe(df, variation_cols, rolling_epoch, predict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWe6CKjUsSQ7"
      },
      "source": [
        "# get the predications and the poi_ids they are for\n",
        "pred_X = predict_df.iloc[:,1:]\n",
        "labels = predict_df.iloc[:,0]\n",
        "preds = clf.predict(pred_X)\n",
        "# predictions are not whole numbers, so round \n",
        "preds = preds.round()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwD1OZ71sSQ7"
      },
      "source": [
        "Writing results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3yGzmNesSQ7"
      },
      "source": [
        "# write to csv file for submission\n",
        "with open(\"results_rf.csv\", \"w\") as f:\n",
        "    f.write(\"poi_id,raw_visit_counts\\n\")\n",
        "    for poi_id, count in zip(labels, preds):\n",
        "        f.write(\"{a},{b}\\n\".format(a=poi_id, b=int(count)))\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkvQ6Y7GsSQ7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}